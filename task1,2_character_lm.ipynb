{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 2. Language modeling.\n",
    "\n",
    "This task is devoted to language modeling. Its goal is to write in PyTorch an RNN-based language model. Since word-based language modeling requires long training and is memory-consuming due to large vocabulary, we start with character-based language modeling. We are going to train the model to generate words as sequence of characters. During training we teach it to predict characters of the words in the training set.\n",
    "\n",
    "\n",
    "\n",
    "## Task 1. Character-based language modeling: data preparation (15 points)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We train the language models on the materials of **Sigmorphon 2018 Shared Task**. First, download the Russian datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2020-03-25 11:58:25--  https://raw.githubusercontent.com/sigmorphon/conll2018/master/task1/surprise/russian-train-high\n",
      "Распознаётся raw.githubusercontent.com (raw.githubusercontent.com)… 151.101.112.133\n",
      "Подключение к raw.githubusercontent.com (raw.githubusercontent.com)|151.101.112.133|:443... соединение установлено.\n",
      "HTTP-запрос отправлен. Ожидание ответа… 200 OK\n",
      "Длина: 533309 (521K) [text/plain]\n",
      "Сохранение в: «russian-train-high»\n",
      "\n",
      "russian-train-high  100%[===================>] 520,81K   350KB/s    за 1,5s    \n",
      "\n",
      "2020-03-25 11:58:27 (350 KB/s) - «russian-train-high» сохранён [533309/533309]\n",
      "\n",
      "--2020-03-25 11:58:27--  https://raw.githubusercontent.com/sigmorphon/conll2018/master/task1/surprise/russian-dev\n",
      "Распознаётся raw.githubusercontent.com (raw.githubusercontent.com)… 151.101.112.133\n",
      "Подключение к raw.githubusercontent.com (raw.githubusercontent.com)|151.101.112.133|:443... соединение установлено.\n",
      "HTTP-запрос отправлен. Ожидание ответа… 200 OK\n",
      "Длина: 53671 (52K) [text/plain]\n",
      "Сохранение в: «russian-dev»\n",
      "\n",
      "russian-dev         100%[===================>]  52,41K  --.-KB/s    за 0,09s   \n",
      "\n",
      "2020-03-25 11:58:28 (562 KB/s) - «russian-dev» сохранён [53671/53671]\n",
      "\n",
      "--2020-03-25 11:58:28--  https://raw.githubusercontent.com/sigmorphon/conll2018/master/task1/surprise/russian-test\n",
      "Распознаётся raw.githubusercontent.com (raw.githubusercontent.com)… 151.101.112.133\n",
      "Подключение к raw.githubusercontent.com (raw.githubusercontent.com)|151.101.112.133|:443... соединение установлено.\n",
      "HTTP-запрос отправлен. Ожидание ответа… 200 OK\n",
      "Длина: 53514 (52K) [text/plain]\n",
      "Сохранение в: «russian-test»\n",
      "\n",
      "russian-test        100%[===================>]  52,26K  --.-KB/s    за 0,1s    \n",
      "\n",
      "2020-03-25 11:58:29 (527 KB/s) - «russian-test» сохранён [53514/53514]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget https://raw.githubusercontent.com/sigmorphon/conll2018/master/task1/surprise/russian-train-high\n",
    "!wget https://raw.githubusercontent.com/sigmorphon/conll2018/master/task1/surprise/russian-dev\n",
    "!wget https://raw.githubusercontent.com/sigmorphon/conll2018/master/task1/surprise/russian-test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1.1 (1 points)**\n",
    "All the files contain tab-separated triples ```<lemma>-<form>-<tags>```, where ```<form>``` may contain spaces (*будете соответствовать*). Write a function that loads a list of all word forms, that do not contain spaces.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_infile(infile):\n",
    "    \"\"\"\n",
    "    == MY CODE HERE ==\n",
    "    \"\"\"\n",
    "    words = []\n",
    "    with open(infile, encoding='utf-8') as f:\n",
    "        lines = f.readlines()\n",
    "        for line in lines:\n",
    "            splited_line = line.split()\n",
    "            if len(splited_line )==3:\n",
    "                words.append(splited_line[1].lower())\n",
    "    return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9213 917 922\n",
      "валлонскому незаконченным истрёпывав личного серьгам необоснованным тюти заросла идеальна гулкой\n"
     ]
    }
   ],
   "source": [
    "train_words = read_infile(\"russian-train-high\")\n",
    "dev_words = read_infile(\"russian-dev\")\n",
    "test_words = read_infile(\"russian-test\")\n",
    "print(len(train_words), len(dev_words), len(test_words))\n",
    "print(*train_words[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1.2 (2 points)** Write a **Vocabulary** class that allows to transform symbols into their indexes. The class should have the method ```__call__``` that applies this transformation to sequences of symbols and batches of sequences as well. You can also use [SimpleVocabulary](https://github.com/deepmipt/DeepPavlov/blob/c10b079b972493220c82a643d47d718d5358c7f4/deeppavlov/core/data/simple_vocab.py#L31) from DeepPavlov. Fit an instance of this class on the training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-03-31 12:27:55.875 WARNING in 'deeppavlov.core.models.serializable'['serializable'] at line 49: No load path is set for SimpleVocabulary in 'infer' mode. Using save path instead\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "37\n"
     ]
    }
   ],
   "source": [
    "from deeppavlov.core.data.simple_vocab import SimpleVocabulary\n",
    "\"\"\"\n",
    "== MY CODE HERE ==\n",
    "\"\"\"\n",
    "vocab = SimpleVocabulary(special_tokens=('PAD', 'BEGIN', 'END'), \n",
    "                         unk_token='UNK', save_path='./tmp')\n",
    "\n",
    "vocab.fit([list(x) for x in train_words])\n",
    "print(len(vocab))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1.3 (2 points)** Write a **Dataset** class, which should be inherited from ```torch.utils.data.Dataset```. It should take a list of words and the ```vocab``` as initialization arguments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset as TorchDataset\n",
    "  \n",
    "class Dataset(TorchDataset):\n",
    "    \"\"\"Custom data.Dataset compatible with data.DataLoader.\"\"\"\n",
    "\n",
    "    def __init__(self, data, vocab):\n",
    "        self.data = data\n",
    "        self.vocab = vocab\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        \"\"\"\n",
    "        Returns one tensor pair (source and target). The source tensor corresponds to the input word,\n",
    "        with \"BEGIN\" and \"END\" symbols attached. The target tensor should contain the answers\n",
    "        for the language model that obtain these word as input.\n",
    "        \"\"\"\n",
    "        word = self.data[index]\n",
    "        batch = self.vocab.__call__(['BEGIN'] + list(word) + ['END'])\n",
    "        return (torch.LongTensor(batch[:-1]), torch.LongTensor(batch[1:]))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device:  cuda\n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.is_available():\n",
    "    device = 'cuda'\n",
    "    torch.cuda.set_device(1)\n",
    "else:\n",
    "    device = 'cpu'\n",
    "\n",
    "\n",
    "print(\"device: \", device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = Dataset(train_words, vocab)\n",
    "dev_dataset = Dataset(dev_words, vocab)\n",
    "test_dataset = Dataset(test_words, vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1.4 (3 points)** Use a standard ```torch.utils.data.DataLoader``` to obtain an iterable over batches. Print the shape of first 10 input batches with ```batch_size=1```."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "\"\"\"\n",
    "== MY CODE HERE ==\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=1)\n",
    "dev_loader = DataLoader(dev_dataset, batch_size=1)\n",
    "test_loader = DataLoader(test_dataset, batch_size=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 12]) torch.Size([1, 12])\n",
      "torch.Size([1, 14]) torch.Size([1, 14])\n",
      "torch.Size([1, 11]) torch.Size([1, 11])\n",
      "torch.Size([1, 8]) torch.Size([1, 8])\n",
      "torch.Size([1, 8]) torch.Size([1, 8])\n",
      "torch.Size([1, 15]) torch.Size([1, 15])\n",
      "torch.Size([1, 5]) torch.Size([1, 5])\n",
      "torch.Size([1, 8]) torch.Size([1, 8])\n",
      "torch.Size([1, 9]) torch.Size([1, 9])\n",
      "torch.Size([1, 7]) torch.Size([1, 7])\n"
     ]
    }
   ],
   "source": [
    "        \n",
    "for batch, i in zip(train_loader, range(10)):\n",
    "    print (batch[0].shape, batch[1].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(1.5) 1 point** Explain, why this does not work with larger batch size."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Because we cannot create batches of tensors with varying dimensions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(1.6) 5 points** Write a function **collate** that allows you to deal with batches of greater size. See [discussion](https://discuss.pytorch.org/t/dataloader-for-various-length-of-data/6418/8) for an example. Implement your function as a class ```__call__``` method to make it more flexible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def pad_tensor(vec, length, dim, pad_symbol):\n",
    "    \"\"\"\n",
    "    Pads a vector ``vec`` up to length ``length`` along axis ``dim`` with pad symbol ``pad_symbol``.\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    == MY CODE HERE ==\n",
    "    \"\"\"\n",
    "    output_dims = list(vec.shape)\n",
    "    pad = np.zeros((len(output_dims)*2,)).astype(int).tolist()\n",
    "    k = len(output_dims)-dim-1\n",
    "    pad[2*k+1] = length-output_dims[dim]\n",
    "    return torch.nn.functional.pad(vec, pad, mode='constant', value=pad_symbol)\n",
    "\n",
    "class Padder:\n",
    "    \n",
    "    def __init__(self, dim=0, pad_symbol=0, length=None):\n",
    "        self.dim = dim\n",
    "        self.pad_symbol = pad_symbol\n",
    "        self.length = length\n",
    "    def __call__(self, batch):\n",
    "        \"\"\"\n",
    "        == MY CODE HERE ==\n",
    "        \"\"\"\n",
    "        if self.length is None:\n",
    "            length = sorted([term[0].shape[self.dim] for term in batch])[-1]\n",
    "        else:\n",
    "            length = self.length\n",
    "        source = torch.stack([pad_tensor(term[0], length, self.dim, self.pad_symbol) for term in batch])\n",
    "        targets = torch.stack([pad_tensor(term[1], length, self.dim, self.pad_symbol) for term in batch])\n",
    "        return [source, targets]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(1.7) 1 points** Again, use ```torch.utils.data.DataLoader``` to obtain an iterable over batches. Print the shape of first 10 input batches with the batch size you like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "\"\"\"\n",
    "== MY CODE HERE ==\n",
    "\"\"\"\n",
    "pad_symb=vocab._t2i['PAD']\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, collate_fn=Padder(pad_symbol=pad_symb))\n",
    "dev_loader = DataLoader(dev_dataset, batch_size=32, collate_fn=Padder(pad_symbol=pad_symb))\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, collate_fn=Padder(pad_symbol=pad_symb))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 16]) torch.Size([32, 16])\n",
      "torch.Size([32, 18]) torch.Size([32, 18])\n",
      "torch.Size([32, 16]) torch.Size([32, 16])\n",
      "torch.Size([32, 17]) torch.Size([32, 17])\n",
      "torch.Size([32, 19]) torch.Size([32, 19])\n",
      "torch.Size([32, 16]) torch.Size([32, 16])\n",
      "torch.Size([32, 17]) torch.Size([32, 17])\n",
      "torch.Size([32, 18]) torch.Size([32, 18])\n",
      "torch.Size([32, 16]) torch.Size([32, 16])\n",
      "torch.Size([32, 19]) torch.Size([32, 19])\n"
     ]
    }
   ],
   "source": [
    "        \n",
    "for batch, i in zip(train_loader, range(10)):\n",
    "    print (batch[0].shape, batch[1].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2. Character-based language modeling. (35 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2.1 (5 points)** Write a network that performs language modeling. It should include three layers:\n",
    "1. **Embedding** layer that transforms input symbols into vectors.\n",
    "2. An **RNN** layer that outputs a sequence of hidden states (you may use https://pytorch.org/docs/stable/nn.html#gru).\n",
    "3. A **Linear** layer with ``softmax`` activation that produces the output distribution for each symbol."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class RNNLM(nn.Module):\n",
    "\n",
    "    def __init__(self, vocab_size, embeddings_dim, hidden_size, batch_size, device):\n",
    "        super(RNNLM, self).__init__()\n",
    "        \"\"\"\n",
    "        == MY CODE HERE ==\n",
    "        \"\"\"\n",
    "        self.vocab_size = vocab_size\n",
    "        self.embeddings_dim = embeddings_dim\n",
    "        self.hidden_size = hidden_size\n",
    "        self.batch_size = batch_size\n",
    "        self.device = device\n",
    "        self.embedding = nn.Embedding(num_embeddings=self.vocab_size, embedding_dim=self.embeddings_dim)\n",
    "        self.gru = nn.GRU(input_size=self.embeddings_dim, hidden_size=self.hidden_size, batch_first=True)\n",
    "        self.linear = nn.Linear(in_features=self.hidden_size, out_features=self.vocab_size)\n",
    "        self.softmax = nn.LogSoftmax(dim=2)#Softmax doesn't work directly with NLLLoss\n",
    "        \n",
    "            \n",
    "        \n",
    "    def forward(self, inputs, hidden=None):\n",
    "        \"\"\"\n",
    "        == MY CODE HERE ==\n",
    "        \"\"\"\n",
    "        if hidden is None:\n",
    "            hidden = self.init_hidden(inputs.shape)\n",
    "        outputs = self.embedding(inputs)\n",
    "        outputs, hidden = self.gru(outputs, hidden)\n",
    "        outputs = self.linear(outputs)\n",
    "        outputs = self.softmax(outputs)\n",
    "        return outputs, hidden\n",
    "\n",
    "    def init_hidden(self, inputs_shape):\n",
    "        return torch.zeros(1, inputs_shape[0], self.hidden_size, device = self.device)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2.2 (1 points)** Write a function ``validate_on_batch`` that takes as input a model, a batch of inputs and a batch of outputs, and the loss criterion, and outputs the loss tensor for the whole batch. This loss should not be normalized."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate_on_batch(model, criterion, x, y):\n",
    "    \"\"\"\n",
    "    == MY CODE HERE ==\n",
    "    \"\"\"\n",
    "    x = x.to(device)\n",
    "    y = y.to(device)\n",
    "    predictions, _ = model(x)\n",
    "    return criterion(predictions.reshape(predictions.shape[0] * predictions.shape[1], -1), \n",
    "                     y.reshape(predictions.shape[1] * predictions.shape[0]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2.3 (1 points)** Write a function ``train_on_batch`` that accepts all the arguments of ``validate_on_batch`` and also an optimizer, calculates loss and makes a single step of gradient optimization. This function should call ``validate_on_batch`` inside."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_on_batch(model, criterion, x, y, optimizer):\n",
    "    \"\"\"\n",
    "    == MY CODE HERE ==\n",
    "    \"\"\"\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    loss = validate_on_batch(model, criterion, x, y)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    return np.mean(loss.item())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2.4 (3 points)** Write a training loop. You should define your ``RNNLM`` model, the criterion, the optimizer and the hyperparameters (number of epochs and batch size). Then train the model for a required number of epochs. On each epoch evaluate the average training loss and the average loss on the validation set. \n",
    "\n",
    "**2.5 (3 points)** Do not forget to average your loss over only non-padding symbols, otherwise it will be too optimistic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RNNLM(\n",
       "  (embedding): Embedding(37, 37)\n",
       "  (gru): GRU(37, 200, batch_first=True)\n",
       "  (linear): Linear(in_features=200, out_features=37, bias=True)\n",
       "  (softmax): LogSoftmax()\n",
       ")"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_size = 32\n",
    "learning_rate = 0.001\n",
    "num_epochs = 10\n",
    "train_loss = []\n",
    "val_loss = []\n",
    "\n",
    "rnnlm = RNNLM(vocab_size=len(vocab), embeddings_dim=len(vocab), hidden_size=200, batch_size=batch_size, device=device)\n",
    "rnnlm.to(device)\n",
    "\n",
    "weights = torch.ones((len(vocab),)).to(device)\n",
    "weights[pad_symb]  = 0\n",
    "criterion = nn.NLLLoss(weight=weights, reduction='mean')\n",
    "optimizer = torch.optim.Adam(rnnlm.parameters(), lr=learning_rate)\n",
    "rnnlm.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: train_loss = 2.541464270816909, val_loss = 2.25935435295105\n",
      "Epoch 1: train_loss = 2.212066263788276, val_loss = 2.136517286300659\n",
      "Epoch 2: train_loss = 2.1097913111249604, val_loss = 2.0814638137817383\n",
      "Epoch 3: train_loss = 2.0398490909073086, val_loss = 2.047314167022705\n",
      "Epoch 4: train_loss = 1.9840044130881627, val_loss = 2.0333330631256104\n",
      "Epoch 5: train_loss = 1.9351658353375063, val_loss = 2.0310568809509277\n",
      "Epoch 6: train_loss = 1.8899972086979284, val_loss = 2.033853530883789\n",
      "Epoch 7: train_loss = 1.847171113308933, val_loss = 2.0389351844787598\n",
      "Epoch 8: train_loss = 1.8061071671545506, val_loss = 2.0434634685516357\n",
      "Epoch 9: train_loss = 1.766682892623875, val_loss = 2.0463063716888428\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(num_epochs):\n",
    "    iterations_train = 0\n",
    "    train_loss_iter = 0\n",
    "    val_loss_iter = 0\n",
    "    for ind, (x, y) in enumerate(train_loader):\n",
    "        train_loss_iter += train_on_batch(rnnlm, criterion, x, y, optimizer)\n",
    "        iterations_train += 1\n",
    "        torch.cuda.empty_cache()\n",
    "    for ind, (x, y) in enumerate(dev_loader):\n",
    "        rnnlm.eval()\n",
    "        val_loss_iter = validate_on_batch(rnnlm, criterion, x, y)\n",
    "        torch.cuda.empty_cache()\n",
    "    train_loss.append(train_loss_iter / iterations_train)\n",
    "    val_loss.append(val_loss_iter)\n",
    "    print(\"Epoch {}: train_loss = {}, val_loss = {}\".format(epoch, train_loss[-1], val_loss[-1]))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.plot(np.arange(1,len(train_loss)+1), train_loss, label='train', color='black')\n",
    "ax.plot(np.arange(1,len(val_loss)+1), val_loss, label='val', color='red')\n",
    "\n",
    "ax.set(ylabel='loss', xlabel='epoch')\n",
    "ax.grid()\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2.6 (5 points)** Write a function **predict_on_batch** that outputs letter probabilities of all words in the batch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "== MY CODE HERE ==\n",
    "\"\"\"\n",
    "\n",
    "def predict_on_batch(model, batch, hidden=None, dim=-1):\n",
    "    model.eval()\n",
    "    batch = batch.to(device)\n",
    "    preds, hidden = model(batch, hidden)\n",
    "    softmax = nn.Softmax(dim=dim)\n",
    "    return (softmax(preds), hidden)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2.7 (1 points)** Calculate the letter probabilities for all words in the test dataset. Print them for 20 last words. Do not forget to disable shuffling in the ``DataLoader``."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "== MY CODE HERE ==\n",
    "\"\"\"\n",
    "length = max(map(lambda x: x[0].shape[-1], test_dataset))\n",
    "test_loader = DataLoader(train_dataset, batch_size=32, shuffle = False, collate_fn=Padder(length=length, pad_symbol=vocab._t2i['END']))\n",
    "probs = None\n",
    "hidden = None\n",
    "\n",
    "for ind, (x, y) in enumerate(test_loader): \n",
    "    if ind == 0:\n",
    "        probs, hidden = predict_on_batch(rnnlm, x)\n",
    "    else:\n",
    "        hidden = hidden[:, :x.shape[0], :]\n",
    "        preds, hidden = predict_on_batch(rnnlm, x, hidden=hidden)\n",
    "        probs = torch.cat((probs, preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "word:  пшеничной\n",
      "letter п : probability 0.03949384018778801\n",
      "letter ш : probability 0.000218803936149925\n",
      "letter е : probability 0.00202156906016171\n",
      "letter н : probability 0.0014598234556615353\n",
      "letter и : probability 0.0007287629996426404\n",
      "letter ч : probability 0.01207261998206377\n",
      "letter н : probability 0.0014139177510514855\n",
      "letter о : probability 0.0005339662311598659\n",
      "letter й : probability 0.011004971340298653\n",
      "word:  переживаниями\n",
      "letter п : probability 0.18422503769397736\n",
      "letter е : probability 8.462680852971971e-05\n",
      "letter р : probability 0.007271329872310162\n",
      "letter е : probability 0.00013792408572044224\n",
      "letter ж : probability 0.04084709286689758\n",
      "letter и : probability 0.0009356595110148191\n",
      "letter в : probability 0.003924857825040817\n",
      "letter а : probability 0.00033628291566856205\n",
      "letter н : probability 0.00036004060530103743\n",
      "letter и : probability 0.00017847590788733214\n",
      "letter я : probability 0.001621716539375484\n",
      "letter м : probability 0.00256390031427145\n",
      "letter и : probability 4.412476118886843e-05\n",
      "word:  наибольшим\n",
      "letter н : probability 0.0007356814458034933\n",
      "letter а : probability 0.02623857744038105\n",
      "letter и : probability 0.0003601578064262867\n",
      "letter б : probability 0.0038848265539854765\n",
      "letter о : probability 0.24396668374538422\n",
      "letter л : probability 0.26983582973480225\n",
      "letter ь : probability 0.06755246222019196\n",
      "letter ш : probability 0.0017339694313704967\n",
      "letter и : probability 0.0014486274449154735\n",
      "letter м : probability 0.0005817727651447058\n",
      "word:  правителями\n",
      "letter п : probability 0.08760774880647659\n",
      "letter р : probability 0.08043505996465683\n",
      "letter а : probability 0.00048466172302141786\n",
      "letter в : probability 0.003502920037135482\n",
      "letter и : probability 0.00022898954921402037\n",
      "letter т : probability 0.0034543005749583244\n",
      "letter е : probability 0.0014486868167296052\n",
      "letter л : probability 0.0356275700032711\n",
      "letter я : probability 0.00474556814879179\n",
      "letter м : probability 0.06870409101247787\n",
      "letter и : probability 9.929781663231552e-05\n",
      "word:  апеллируешь\n",
      "letter а : probability 0.0008573450613766909\n",
      "letter п : probability 0.055638354271650314\n",
      "letter е : probability 0.003925679717212915\n",
      "letter л : probability 0.002758788876235485\n",
      "letter л : probability 0.00017224157636519521\n",
      "letter и : probability 0.00032636799733154476\n",
      "letter р : probability 0.005392529536038637\n",
      "letter у : probability 6.551299156853929e-05\n",
      "letter е : probability 0.0011030473979189992\n",
      "letter ш : probability 0.012554630637168884\n",
      "letter ь : probability 0.008555259555578232\n",
      "word:  односложных\n",
      "letter о : probability 0.014382611028850079\n",
      "letter д : probability 0.004908370319753885\n",
      "letter н : probability 0.0012927672360092402\n",
      "letter о : probability 0.006727965082973242\n",
      "letter с : probability 0.02791241556406021\n",
      "letter л : probability 0.05421612784266472\n",
      "letter о : probability 0.023322679102420807\n",
      "letter ж : probability 0.01730157807469368\n",
      "letter н : probability 0.03459868207573891\n",
      "letter ы : probability 2.6121522751054727e-05\n",
      "letter х : probability 0.019447114318609238\n",
      "word:  колодками\n",
      "letter к : probability 0.0045217061415314674\n",
      "letter о : probability 0.6183409690856934\n",
      "letter л : probability 0.01551511138677597\n",
      "letter о : probability 0.07328204810619354\n",
      "letter д : probability 0.0009819073602557182\n",
      "letter к : probability 0.0008773138979449868\n",
      "letter а : probability 0.0020310848485678434\n",
      "letter м : probability 0.004364361520856619\n",
      "letter и : probability 0.0021689163986593485\n",
      "word:  вибрирует\n",
      "letter в : probability 0.0053943912498652935\n",
      "letter и : probability 0.0012037192936986685\n",
      "letter б : probability 0.130280539393425\n",
      "letter р : probability 0.0663284957408905\n",
      "letter и : probability 0.00017665268387645483\n",
      "letter р : probability 0.25198328495025635\n",
      "letter у : probability 0.008481555618345737\n",
      "letter е : probability 0.006463515106588602\n",
      "letter т : probability 0.029309730976819992\n",
      "word:  безразлично\n",
      "letter б : probability 0.05246239900588989\n",
      "letter е : probability 0.001971706049516797\n",
      "letter з : probability 0.025151025503873825\n",
      "letter р : probability 0.003801054321229458\n",
      "letter а : probability 0.0009691780433058739\n",
      "letter з : probability 0.020396187901496887\n",
      "letter л : probability 0.0011487988522276282\n",
      "letter и : probability 0.01777525804936886\n",
      "letter ч : probability 0.009633866138756275\n",
      "letter н : probability 0.006942213978618383\n",
      "letter о : probability 0.42959901690483093\n",
      "word:  эпитрохоиду\n",
      "letter э : probability 0.07146702706813812\n",
      "letter п : probability 0.08229708671569824\n",
      "letter и : probability 0.0004160724929533899\n",
      "letter т : probability 0.0028256825171411037\n",
      "letter р : probability 0.010752016678452492\n",
      "letter о : probability 0.0033647436648607254\n",
      "letter х : probability 0.006442463956773281\n",
      "letter о : probability 0.0014749332331120968\n",
      "letter и : probability 0.000750943087041378\n",
      "letter д : probability 0.04636700078845024\n",
      "letter у : probability 0.00903974287211895\n",
      "word:  общеобразовательного\n",
      "letter о : probability 0.005721531808376312\n",
      "letter б : probability 0.018490847200155258\n",
      "letter щ : probability 0.0063683222979307175\n",
      "letter е : probability 0.006563266739249229\n",
      "letter о : probability 0.07404822111129761\n",
      "letter б : probability 0.0895765870809555\n",
      "letter р : probability 0.0020767501555383205\n",
      "letter а : probability 0.03388480842113495\n",
      "letter з : probability 0.07830876111984253\n",
      "letter о : probability 0.04103199392557144\n",
      "letter в : probability 0.033011097460985184\n",
      "letter а : probability 0.0017363469814881682\n",
      "letter т : probability 0.058881063014268875\n",
      "letter е : probability 0.012730182148516178\n",
      "letter л : probability 0.0015560114989057183\n",
      "letter ь : probability 0.004675781354308128\n",
      "letter н : probability 0.0008274380816146731\n",
      "letter о : probability 0.00030851055635139346\n",
      "letter г : probability 0.009870532900094986\n",
      "letter о : probability 0.00028792707598768175\n",
      "word:  фригидной\n",
      "letter ф : probability 0.07851193100214005\n",
      "letter р : probability 0.061637602746486664\n",
      "letter и : probability 0.0010382120963186026\n",
      "letter г : probability 0.03236859664320946\n",
      "letter и : probability 0.0061017959378659725\n",
      "letter д : probability 0.15138646960258484\n",
      "letter н : probability 0.0019087388645857573\n",
      "letter о : probability 0.0004194819775875658\n",
      "letter й : probability 0.0008414594340138137\n",
      "word:  безмолвный\n",
      "letter б : probability 0.011069769971072674\n",
      "letter е : probability 0.18946729600429535\n",
      "letter з : probability 0.06137959286570549\n",
      "letter м : probability 0.002159474417567253\n",
      "letter о : probability 0.0014266165671870112\n",
      "letter л : probability 0.0035559574607759714\n",
      "letter в : probability 0.026033218950033188\n",
      "letter н : probability 0.03864976391196251\n",
      "letter ы : probability 0.301909476518631\n",
      "letter й : probability 0.017126355320215225\n",
      "word:  многолетним\n",
      "letter м : probability 0.06140986829996109\n",
      "letter н : probability 0.0003029514045920223\n",
      "letter о : probability 0.005936333443969488\n",
      "letter г : probability 0.07919004559516907\n",
      "letter о : probability 0.030966227874159813\n",
      "letter л : probability 0.0077712577767670155\n",
      "letter е : probability 0.008187905885279179\n",
      "letter т : probability 0.06547871232032776\n",
      "letter н : probability 0.00017533008940517902\n",
      "letter и : probability 4.987544525647536e-05\n",
      "letter м : probability 4.516726619385736e-07\n",
      "word:  оттопырьте\n",
      "letter о : probability 0.0014931740006431937\n",
      "letter т : probability 0.0282550361007452\n",
      "letter т : probability 0.0034448045771569014\n",
      "letter о : probability 0.0034908605739474297\n",
      "letter п : probability 0.10367998480796814\n",
      "letter ы : probability 0.0002859119849745184\n",
      "letter р : probability 0.0012841502903029323\n",
      "letter ь : probability 0.003206777386367321\n",
      "letter т : probability 0.009456450119614601\n",
      "letter е : probability 0.006510408129543066\n",
      "word:  долбануть\n",
      "letter д : probability 0.05664026737213135\n",
      "letter о : probability 0.015727650374174118\n",
      "letter л : probability 0.0533452108502388\n",
      "letter б : probability 0.05273671820759773\n",
      "letter а : probability 6.022753586876206e-05\n",
      "letter н : probability 0.11686824262142181\n",
      "letter у : probability 0.0005150922806933522\n",
      "letter т : probability 0.004355152137577534\n",
      "letter ь : probability 0.0005786950350739062\n",
      "word:  синеватые\n",
      "letter с : probability 0.005985839758068323\n",
      "letter и : probability 0.0009441165602765977\n",
      "letter н : probability 0.004350410308688879\n",
      "letter е : probability 0.002653590403497219\n",
      "letter в : probability 0.014716141857206821\n",
      "letter а : probability 0.12195207923650742\n",
      "letter т : probability 0.011465939693152905\n",
      "letter ы : probability 7.317350537050515e-05\n",
      "letter е : probability 0.0027600631583482027\n",
      "word:  колониальному\n",
      "letter к : probability 0.03123641386628151\n",
      "letter о : probability 0.0027236558962613344\n",
      "letter л : probability 0.02013397589325905\n",
      "letter о : probability 0.00658957427367568\n",
      "letter н : probability 0.0016599615337327123\n",
      "letter и : probability 0.016768530011177063\n",
      "letter а : probability 0.015996990725398064\n",
      "letter л : probability 0.02455877885222435\n",
      "letter ь : probability 0.009469973854720592\n",
      "letter н : probability 0.01324846874922514\n",
      "letter о : probability 0.054800231009721756\n",
      "letter м : probability 0.033924978226423264\n",
      "letter у : probability 0.0009329367894679308\n",
      "word:  надавливало\n",
      "letter н : probability 0.0018067327328026295\n",
      "letter а : probability 0.004062084946781397\n",
      "letter д : probability 0.007044485304504633\n",
      "letter а : probability 0.009551405906677246\n",
      "letter в : probability 0.023663230240345\n",
      "letter л : probability 0.02478482946753502\n",
      "letter и : probability 0.0005833075265400112\n",
      "letter в : probability 0.01476122997701168\n",
      "letter а : probability 0.004364639520645142\n",
      "letter л : probability 0.02699686400592327\n",
      "letter о : probability 0.001093587139621377\n",
      "word:  истерический\n",
      "letter и : probability 0.0030836721416562796\n",
      "letter с : probability 0.44205325841903687\n",
      "letter т : probability 0.003703662659972906\n",
      "letter е : probability 0.07914704084396362\n",
      "letter р : probability 0.16999691724777222\n",
      "letter и : probability 0.163169264793396\n",
      "letter ч : probability 0.02491346374154091\n",
      "letter е : probability 0.006390619091689587\n",
      "letter с : probability 0.010996978729963303\n",
      "letter к : probability 0.0166254173964262\n",
      "letter и : probability 0.00028139218920841813\n",
      "letter й : probability 0.02320016920566559\n"
     ]
    }
   ],
   "source": [
    "for i in range(-20, 0):\n",
    "    word = test_dataset.data[i]\n",
    "    print(\"word: \", word)\n",
    "    for j, letter in enumerate(word):\n",
    "        print('letter {} : probability {}'.format(letter, probs[i, j, vocab(letter)]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2.8 (5 points)** Write a function that generates a single word (sequence of indexes) given the model. Do not forget about the hidden state! Be careful about start and end symbol indexes. Use ``torch.multinomial`` for sampling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate(model, max_length=20, start_index=vocab._t2i['BEGIN'],  end_index=vocab._t2i['END'], pad_index=vocab._t2i['PAD']):\n",
    "    \"\"\"\n",
    "    == MY CODE HERE ==\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    previous_letter = start_index\n",
    "    word = []\n",
    "    hidden=None\n",
    "    \n",
    "    for i in range(length):\n",
    "        ind = torch.tensor([previous_letter])[None, :]\n",
    "        prob, hidden = predict_on_batch(model, ind, hidden)\n",
    "        previous_letter = torch.multinomial(prob.squeeze(), 1)\n",
    "        if previous_letter in [start_index, end_index, pad_index]:\n",
    "            return ''.join(word)\n",
    "        previous_letter = previous_letter.cpu().numpy()[0]\n",
    "        word.append(vocab(previous_letter))\n",
    "    return ''.join(word)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2.9 (1 points)** Use ``generate`` to sample 20 pseudowords. Do not forget to transform indexes to letters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "стригрикам\n",
      "галийским\n",
      "гронятию\n",
      "осладностями\n",
      "зиопакталось\n",
      "параинами\n",
      "пролютию\n",
      "разрасту\n",
      "гащу\n",
      "крюрию\n",
      "сосра\n",
      "навосовавшей\n",
      "соиствере\n",
      "эгоубственной\n",
      "полытем\n",
      "нагошением\n",
      "рухех\n",
      "потреговыми\n",
      "обтьёрных\n",
      "нес-жыстеее\n"
     ]
    }
   ],
   "source": [
    "for i in range(20):\n",
    "    \"\"\"\n",
    "    == MY CODE HERE ==\n",
    "    \"\"\"\n",
    "    print(generate(rnnlm))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(2.10) 5 points** Write a batched version of the generation function. You should sample the following symbol only for the words that are not finished yet, so apply a boolean mask to trace active words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_batch(model, batch_size, max_length=20,start_index=vocab._t2i['BEGIN'],  end_index=vocab._t2i['END'], pad_index=vocab._t2i['PAD']):\n",
    "    \"\"\"\n",
    "    == MY CODE HERE ==\n",
    "    \"\"\"\n",
    "    state = torch.tensor([start_index for i in range(batch_size)]).reshape(batch_size, 1)\n",
    "    generated_words = []\n",
    "    generated = None\n",
    "    hidden = None\n",
    "    \n",
    "    for i in range(max_length):\n",
    "        probs, hidden = predict_on_batch(model, state, hidden)\n",
    "        state = torch.multinomial(probs.squeeze(), num_samples=1)\n",
    "        if i == 0:\n",
    "            generated = state[None, :]\n",
    "        else:\n",
    "            generated = torch.cat((generated, state[None, :]))\n",
    "    for i in range(batch_size):\n",
    "        tensor = generated[:, i, 0]\n",
    "        last_pos = max_length\n",
    "        for j in range(max_length):\n",
    "            if tensor[j] in [start_index, end_index, pad_index]:\n",
    "                last_pos = j\n",
    "                break\n",
    "        generated_words.append(''.join([vocab(val.item()) for val in tensor[:last_pos]]))\n",
    "    return generated_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['оглупическим',\n",
       " 'проводишь',\n",
       " 'произнечатов',\n",
       " 'сиской',\n",
       " 'патосной',\n",
       " 'распузанях',\n",
       " 'наглятов',\n",
       " 'бессошали',\n",
       " 'гословостом',\n",
       " 'знойному',\n",
       " 'социедом',\n",
       " 'кираете',\n",
       " 'преспревалит',\n",
       " 'посчитывают',\n",
       " 'единистом',\n",
       " 'абантному',\n",
       " 'якрысой',\n",
       " 'обидывала',\n",
       " 'сужешь',\n",
       " 'мектремского']"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_batch(rnnlm, 20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(2.11) 5 points** Experiment with the type of RNN, number of layers, units and/or dropout to improve the perplexity of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNNLM(nn.Module):\n",
    "\n",
    "    def __init__(self, vocab_size, embeddings_dim, hidden_size1, hidden_size2, batch_size, device):\n",
    "        super(RNNLM, self).__init__()\n",
    "        \"\"\"\n",
    "        == MY CODE HERE ==\n",
    "        \"\"\"\n",
    "        self.vocab_size = vocab_size\n",
    "        self.embeddings_dim = embeddings_dim\n",
    "        self.hidden_size1 = hidden_size1\n",
    "        self.hidden_size2 = hidden_size2\n",
    "        self.batch_size = batch_size\n",
    "        self.device = device\n",
    "        self.embedding = nn.Embedding(num_embeddings=self.vocab_size, embedding_dim=self.embeddings_dim)\n",
    "        self.gru = nn.GRU(input_size=self.embeddings_dim, hidden_size=self.hidden_size1, batch_first=True)\n",
    "        self.dropout1 = nn.Dropout(0.5)\n",
    "        self.linear1 = nn.Linear(in_features=self.hidden_size1, out_features=self.hidden_size2)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.dropout2 = nn.Dropout(0.5)\n",
    "        self.linear2 = nn.Linear(in_features=self.hidden_size2, out_features=self.vocab_size)\n",
    "        self.softmax = nn.LogSoftmax(dim=2)#Softmax doesn't work directly with NLLLoss\n",
    "        \n",
    "            \n",
    "        \n",
    "    def forward(self, inputs, hidden=None):\n",
    "        \"\"\"\n",
    "        == MY CODE HERE ==\n",
    "        \"\"\"\n",
    "        if hidden is None:\n",
    "            hidden = self.init_hidden(inputs.shape)\n",
    "        outputs = self.embedding(inputs)\n",
    "        outputs, hidden = self.gru(outputs, hidden)\n",
    "        outputs = self.dropout1(outputs)\n",
    "        outputs = self.linear1(outputs)\n",
    "        outputs = self.relu(outputs)\n",
    "        outputs = self.dropout2(outputs)\n",
    "        outputs = self.linear2(outputs)\n",
    "        outputs = self.softmax(outputs)\n",
    "        return outputs, hidden\n",
    "\n",
    "    def init_hidden(self, inputs_shape):\n",
    "        return torch.zeros(1, inputs_shape[0], self.hidden_size1, device = self.device) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RNNLM(\n",
       "  (embedding): Embedding(37, 37)\n",
       "  (gru): GRU(37, 100, batch_first=True)\n",
       "  (dropout1): Dropout(p=0.5)\n",
       "  (linear1): Linear(in_features=100, out_features=200, bias=True)\n",
       "  (relu): ReLU()\n",
       "  (dropout2): Dropout(p=0.5)\n",
       "  (linear2): Linear(in_features=200, out_features=37, bias=True)\n",
       "  (softmax): LogSoftmax()\n",
       ")"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_size = 32\n",
    "learning_rate = 0.00001\n",
    "num_epochs = 100\n",
    "train_loss = []\n",
    "val_loss = []\n",
    "\n",
    "rnnlm = RNNLM(vocab_size=len(vocab), embeddings_dim=len(vocab), hidden_size1=100, hidden_size2=200, batch_size=batch_size, device=device)\n",
    "rnnlm.to(device)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(rnnlm.parameters(), lr=learning_rate)\n",
    "rnnlm.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = DataLoader(train_dataset, batch_size=32, collate_fn=Padder(dim=0, pad_symbol=pad_symb))\n",
    "val_dataloader = DataLoader(dev_dataset, batch_size=32, collate_fn=Padder(dim=0, pad_symbol=pad_symb))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: train_loss = 3.5470201869805655, val_loss = 3.358273506164551\n",
      "Epoch 1: train_loss = 3.263772091931767, val_loss = 2.8072407245635986\n",
      "Epoch 2: train_loss = 2.7440136981507144, val_loss = 2.0095531940460205\n",
      "Epoch 3: train_loss = 2.34635386036502, val_loss = 1.7815905809402466\n",
      "Epoch 4: train_loss = 2.2263918080263667, val_loss = 1.7214616537094116\n",
      "Epoch 5: train_loss = 2.169568278723293, val_loss = 1.6815204620361328\n",
      "Epoch 6: train_loss = 2.1228463078538575, val_loss = 1.6443058252334595\n",
      "Epoch 7: train_loss = 2.077665224671364, val_loss = 1.6077706813812256\n",
      "Epoch 8: train_loss = 2.0364380044241748, val_loss = 1.574187159538269\n",
      "Epoch 9: train_loss = 2.0000431194073625, val_loss = 1.54642653465271\n",
      "Epoch 10: train_loss = 1.971457689586613, val_loss = 1.5252177715301514\n",
      "Epoch 11: train_loss = 1.9488770986596744, val_loss = 1.5084068775177002\n",
      "Epoch 12: train_loss = 1.928994607180357, val_loss = 1.494193196296692\n",
      "Epoch 13: train_loss = 1.9126790232128568, val_loss = 1.48121976852417\n",
      "Epoch 14: train_loss = 1.8956330658660994, val_loss = 1.4686026573181152\n",
      "Epoch 15: train_loss = 1.8803112448917494, val_loss = 1.4561583995819092\n",
      "Epoch 16: train_loss = 1.8636155149175062, val_loss = 1.4437886476516724\n",
      "Epoch 17: train_loss = 1.8476879389749632, val_loss = 1.431498646736145\n",
      "Epoch 18: train_loss = 1.8321672872536712, val_loss = 1.4194337129592896\n",
      "Epoch 19: train_loss = 1.8168606186906497, val_loss = 1.4071677923202515\n",
      "Epoch 20: train_loss = 1.8017481002542708, val_loss = 1.3952139616012573\n",
      "Epoch 21: train_loss = 1.7870545859138172, val_loss = 1.3836586475372314\n",
      "Epoch 22: train_loss = 1.7728226263489988, val_loss = 1.3727227449417114\n",
      "Epoch 23: train_loss = 1.7616423074569967, val_loss = 1.3627082109451294\n",
      "Epoch 24: train_loss = 1.7481200127965875, val_loss = 1.3530917167663574\n",
      "Epoch 25: train_loss = 1.7361513781878684, val_loss = 1.344077229499817\n",
      "Epoch 26: train_loss = 1.72676561648647, val_loss = 1.335924744606018\n",
      "Epoch 27: train_loss = 1.7171600552068815, val_loss = 1.329058051109314\n",
      "Epoch 28: train_loss = 1.7085772396789656, val_loss = 1.3226587772369385\n",
      "Epoch 29: train_loss = 1.701268534693453, val_loss = 1.31680166721344\n",
      "Epoch 30: train_loss = 1.691882220407327, val_loss = 1.3111612796783447\n",
      "Epoch 31: train_loss = 1.68417980397741, val_loss = 1.3061044216156006\n",
      "Epoch 32: train_loss = 1.6781805128686957, val_loss = 1.3012171983718872\n",
      "Epoch 33: train_loss = 1.6715845113827124, val_loss = 1.2970242500305176\n",
      "Epoch 34: train_loss = 1.6655581937068038, val_loss = 1.293095350265503\n",
      "Epoch 35: train_loss = 1.6598237624598875, val_loss = 1.2894474267959595\n",
      "Epoch 36: train_loss = 1.6552638490166929, val_loss = 1.2858860492706299\n",
      "Epoch 37: train_loss = 1.6511958398752742, val_loss = 1.282530665397644\n",
      "Epoch 38: train_loss = 1.6453320669631164, val_loss = 1.2793622016906738\n",
      "Epoch 39: train_loss = 1.641842819750309, val_loss = 1.2765729427337646\n",
      "Epoch 40: train_loss = 1.6367133951021566, val_loss = 1.273532748222351\n",
      "Epoch 41: train_loss = 1.6342410383125145, val_loss = 1.2709425687789917\n",
      "Epoch 42: train_loss = 1.6265533185667462, val_loss = 1.2682936191558838\n",
      "Epoch 43: train_loss = 1.624915926820702, val_loss = 1.2659051418304443\n",
      "Epoch 44: train_loss = 1.6210129538344011, val_loss = 1.2637481689453125\n",
      "Epoch 45: train_loss = 1.6172903548512194, val_loss = 1.2616121768951416\n",
      "Epoch 46: train_loss = 1.6157321230404906, val_loss = 1.259319543838501\n",
      "Epoch 47: train_loss = 1.6104048407740064, val_loss = 1.2572332620620728\n",
      "Epoch 48: train_loss = 1.6084202205141385, val_loss = 1.2552543878555298\n",
      "Epoch 49: train_loss = 1.604555945429537, val_loss = 1.2533738613128662\n",
      "Epoch 50: train_loss = 1.6016670155028503, val_loss = 1.2513469457626343\n",
      "Epoch 51: train_loss = 1.5987366787675354, val_loss = 1.2495752573013306\n",
      "Epoch 52: train_loss = 1.5970051263769467, val_loss = 1.2480589151382446\n",
      "Epoch 53: train_loss = 1.5936305394603147, val_loss = 1.2460459470748901\n",
      "Epoch 54: train_loss = 1.5918788100696273, val_loss = 1.244516372680664\n",
      "Epoch 55: train_loss = 1.5893623609509733, val_loss = 1.2429664134979248\n",
      "Epoch 56: train_loss = 1.5853672584311829, val_loss = 1.2412739992141724\n",
      "Epoch 57: train_loss = 1.5845318300028641, val_loss = 1.2396377325057983\n",
      "Epoch 58: train_loss = 1.5823189773493342, val_loss = 1.238065242767334\n",
      "Epoch 59: train_loss = 1.579849021302329, val_loss = 1.236713171005249\n",
      "Epoch 60: train_loss = 1.5796444053865142, val_loss = 1.2351704835891724\n",
      "Epoch 61: train_loss = 1.5763933896604512, val_loss = 1.2335822582244873\n",
      "Epoch 62: train_loss = 1.5733231788294182, val_loss = 1.2320842742919922\n",
      "Epoch 63: train_loss = 1.5708992675774627, val_loss = 1.2306663990020752\n",
      "Epoch 64: train_loss = 1.568730407084028, val_loss = 1.229172706604004\n",
      "Epoch 65: train_loss = 1.566625991422269, val_loss = 1.227744221687317\n",
      "Epoch 66: train_loss = 1.5669752696735992, val_loss = 1.2262381315231323\n",
      "Epoch 67: train_loss = 1.5632892733232842, val_loss = 1.2246993780136108\n",
      "Epoch 68: train_loss = 1.5624622222450044, val_loss = 1.223185658454895\n",
      "Epoch 69: train_loss = 1.560729895821876, val_loss = 1.2219510078430176\n",
      "Epoch 70: train_loss = 1.559627642441127, val_loss = 1.2202824354171753\n",
      "Epoch 71: train_loss = 1.5567882696373596, val_loss = 1.2192777395248413\n",
      "Epoch 72: train_loss = 1.5553567326731152, val_loss = 1.2179594039916992\n",
      "Epoch 73: train_loss = 1.5524428617209196, val_loss = 1.2162104845046997\n",
      "Epoch 74: train_loss = 1.5526489127013419, val_loss = 1.2148070335388184\n",
      "Epoch 75: train_loss = 1.551177326382862, val_loss = 1.2134311199188232\n",
      "Epoch 76: train_loss = 1.548573451116681, val_loss = 1.212250828742981\n",
      "Epoch 77: train_loss = 1.5480592120438814, val_loss = 1.2112535238265991\n",
      "Epoch 78: train_loss = 1.5453711474935214, val_loss = 1.2100080251693726\n",
      "Epoch 79: train_loss = 1.5448512385288875, val_loss = 1.208791971206665\n",
      "Epoch 80: train_loss = 1.5432344105922513, val_loss = 1.207532525062561\n",
      "Epoch 81: train_loss = 1.541175649397903, val_loss = 1.2063374519348145\n",
      "Epoch 82: train_loss = 1.5412149416903655, val_loss = 1.2050650119781494\n",
      "Epoch 83: train_loss = 1.5404191592501268, val_loss = 1.2040026187896729\n",
      "Epoch 84: train_loss = 1.5375791440407436, val_loss = 1.2023802995681763\n",
      "Epoch 85: train_loss = 1.5370537435842886, val_loss = 1.2013417482376099\n",
      "Epoch 86: train_loss = 1.5351908095180988, val_loss = 1.2002427577972412\n",
      "Epoch 87: train_loss = 1.534041869143645, val_loss = 1.1990768909454346\n",
      "Epoch 88: train_loss = 1.5340898452947538, val_loss = 1.197772741317749\n",
      "Epoch 89: train_loss = 1.5321483970102336, val_loss = 1.1967097520828247\n",
      "Epoch 90: train_loss = 1.5297133552117481, val_loss = 1.1957993507385254\n",
      "Epoch 91: train_loss = 1.5292603245211973, val_loss = 1.1948010921478271\n",
      "Epoch 92: train_loss = 1.528855134629541, val_loss = 1.193747878074646\n",
      "Epoch 93: train_loss = 1.527382471288244, val_loss = 1.192537546157837\n",
      "Epoch 94: train_loss = 1.5251985908382468, val_loss = 1.1915478706359863\n",
      "Epoch 95: train_loss = 1.526009465671248, val_loss = 1.1903916597366333\n",
      "Epoch 96: train_loss = 1.5239923637774255, val_loss = 1.189586877822876\n",
      "Epoch 97: train_loss = 1.5232838085956044, val_loss = 1.1884040832519531\n",
      "Epoch 98: train_loss = 1.5234385552919574, val_loss = 1.1876556873321533\n",
      "Epoch 99: train_loss = 1.5206020215733185, val_loss = 1.1862751245498657\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(num_epochs):\n",
    "    iterations_train = 0\n",
    "    train_loss_iter = 0\n",
    "    val_loss_iter = 0\n",
    "    for ind, (x, y) in enumerate(train_loader):\n",
    "        train_loss_iter += train_on_batch(rnnlm, criterion, x, y, optimizer)\n",
    "        iterations_train += 1\n",
    "        torch.cuda.empty_cache()\n",
    "    for ind, (x, y) in enumerate(dev_loader):\n",
    "        rnnlm.eval()\n",
    "        val_loss_iter = validate_on_batch(rnnlm, criterion, x, y)\n",
    "        torch.cuda.empty_cache()\n",
    "    train_loss.append(train_loss_iter / iterations_train)\n",
    "    val_loss.append(val_loss_iter)\n",
    "    print(\"Epoch {}: train_loss = {}, val_loss = {}\".format(epoch, train_loss[-1], val_loss[-1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, ax = plt.subplots()\n",
    "ax.plot(np.arange(1,len(train_loss)+1), train_loss, label='train', color='black')\n",
    "ax.plot(np.arange(1,len(val_loss)+1), val_loss, label='val', color='red')\n",
    "\n",
    "ax.set(ylabel='loss', xlabel='epoch')\n",
    "ax.grid()\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['чальных',\n",
       " 'попония',\n",
       " 'торбалися',\n",
       " 'попезнобнам',\n",
       " 'рнутеля',\n",
       " 'тезлеломы',\n",
       " 'хрестажитие',\n",
       " 'келъуты',\n",
       " 'ижавёнкый',\n",
       " 'зрижьскыли',\n",
       " 'памтегльело',\n",
       " 'ядушисловшеваелия',\n",
       " 'убакрокаатит',\n",
       " 'уитеслаторем',\n",
       " 'дестрыры',\n",
       " 'вубезжнох',\n",
       " 'певелфшуе',\n",
       " 'пенёльноим',\n",
       " 'одотлюле',\n",
       " 'келяльн']"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_batch(rnnlm, 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
