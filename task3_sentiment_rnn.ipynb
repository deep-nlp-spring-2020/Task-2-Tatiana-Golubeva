{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assignment 2.3: Text classification via RNN (30 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this assignment you will perform sentiment analysis of the IMDBs reviews by using RNN. An additional goal is to learn high abstactions of the **torchtext** module that consists of data processing utilities and popular datasets for natural language."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "from torchtext import datasets\n",
    "\n",
    "from torchtext.data import Field, LabelField\n",
    "from torchtext.data import BucketIterator\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device:  cuda\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "if torch.cuda.is_available():\n",
    "    device = 'cuda'\n",
    "    torch.cuda.set_device(3)\n",
    "else:\n",
    "    device = 'cpu'\n",
    "\n",
    "\n",
    "print(\"device: \", device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.current_device()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preparing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEXT = Field(sequential=True, lower=True)\n",
    "LABEL = LabelField()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, tst = datasets.IMDB.splits(TEXT, LABEL)\n",
    "trn, vld = train.split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1.45 s, sys: 126 ms, total: 1.58 s\n",
      "Wall time: 1.88 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "TEXT.build_vocab(trn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "LABEL.build_vocab(trn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The vocab.freqs is a collections.Counter object, so we can take a look at the most frequent words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('the', 227670),\n",
       " ('a', 112248),\n",
       " ('and', 111447),\n",
       " ('of', 102003),\n",
       " ('to', 94477),\n",
       " ('is', 73358),\n",
       " ('in', 63743),\n",
       " ('i', 49263),\n",
       " ('this', 48896),\n",
       " ('that', 46565)]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TEXT.vocab.freqs.most_common(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TEXT vocabulary size: 202507\n",
      "LABEL vocabulary size: 2\n"
     ]
    }
   ],
   "source": [
    "print(\"TEXT vocabulary size:\",len(TEXT.vocab))\n",
    "print(\"LABEL vocabulary size:\",len(LABEL.vocab))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating the Iterator (2 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "During training, we'll be using a special kind of Iterator, called the **BucketIterator**. When we pass data into a neural network, we want the data to be padded to be the same length so that we can process them in batch:\n",
    "\n",
    "e.g.\n",
    "\\[ \n",
    "\\[3, 15, 2, 7\\],\n",
    "\\[4, 1\\], \n",
    "\\[5, 5, 6, 8, 1\\] \n",
    "\\] -> \\[ \n",
    "\\[3, 15, 2, 7, **0**\\],\n",
    "\\[4, 1, **0**, **0**, **0**\\], \n",
    "\\[5, 5, 6, 8, 1\\] \n",
    "\\] \n",
    "\n",
    "If the sequences differ greatly in length, the padding will consume a lot of wasteful memory and time. The BucketIterator groups sequences of similar lengths together for each batch to minimize padding.\n",
    "\n",
    "Complete the definition of the **BucketIterator** object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_iter, val_iter, test_iter = BucketIterator.splits(\n",
    "        (trn, vld, tst),\n",
    "        batch_sizes=(64, 64, 64),\n",
    "        sort=False,\n",
    "        sort_key=lambda x: len(x.text),\n",
    "        sort_within_batch=False,\n",
    "        device='cuda',\n",
    "        repeat=False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a look at what the output of the BucketIterator looks like. Do not be suprised **batch_first=True**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 2294,    10, 21966,  ...,     9,    10,     2],\n",
       "        [   10,     7,   310,  ...,   266,    20,   331],\n",
       "        [   14,     3,    34,  ...,    10,    74,   329],\n",
       "        ...,\n",
       "        [    1,     1,     1,  ...,     1,     1,     1],\n",
       "        [    1,     1,     1,  ...,     1,     1,     1],\n",
       "        [    1,     1,     1,  ...,     1,     1,     1]], device='cuda:3')"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch = next(train_iter.__iter__()); batch.text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The batch has all the fields we passed to the Dataset as attributes. The batch data can be accessed through the attribute with the same name."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['batch_size', 'dataset', 'fields', 'input_fields', 'target_fields', 'text', 'label'])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch.__dict__.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define the RNN-based text classification model (10 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Start simple first. Implement the model according to the shema below.  \n",
    "![alt text](https://miro.medium.com/max/1396/1*v-tLYQCsni550A-hznS0mw.jpeg)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNNBaseline(nn.Module):\n",
    "    def __init__(self, vocab_size, hidden_dim, emb_dim, n_layers, device=device):\n",
    "        super().__init__()\n",
    "        self.vocab_size = vocab_size\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.emb_dim = emb_dim\n",
    "        self.n_layers = n_layers\n",
    "        self.embedding = nn.Embedding(num_embeddings=self.vocab_size, embedding_dim=self.emb_dim)\n",
    "        self.gru = nn.GRU(self.emb_dim, self.hidden_dim, num_layers=self.n_layers, dropout=0.5, bidirectional=True)\n",
    "        self.linear = nn.Linear(in_features=self.hidden_dim*2, out_features=1)\n",
    "        self.act = nn.Sigmoid()\n",
    "            \n",
    "    def forward(self, seq):\n",
    "        \n",
    "        emb = self.embedding(seq)\n",
    "        outputs, hidden = self.gru(emb)\n",
    "        hidden = torch.cat((hidden[-2,:,:], hidden[-1,:,:]), dim = 1)\n",
    "        outputs = self.linear(hidden)\n",
    "        preds = self.act(outputs)\n",
    "\n",
    "        return preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RNNBaseline(\n",
       "  (embedding): Embedding(201435, 200)\n",
       "  (gru): GRU(200, 300, num_layers=2, dropout=0.5, bidirectional=True)\n",
       "  (linear): Linear(in_features=600, out_features=1, bias=True)\n",
       "  (act): Sigmoid()\n",
       ")"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "em_sz = 200\n",
    "nh = 300\n",
    "vocab_size = len(TEXT.vocab)\n",
    "num_layers = 2\n",
    "\n",
    "model = RNNBaseline(vocab_size=vocab_size, hidden_dim=nh, emb_dim=em_sz, n_layers=num_layers, device=device); model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you're using a GPU, remember to call model.cuda() to move your model to the GPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RNNBaseline(\n",
       "  (embedding): Embedding(201435, 200)\n",
       "  (gru): GRU(200, 300, num_layers=2, dropout=0.5, bidirectional=True)\n",
       "  (linear): Linear(in_features=600, out_features=1, bias=True)\n",
       "  (act): Sigmoid()\n",
       ")"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.cuda()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The training loop (3 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the optimization and the loss functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt = optim.Adam(model.parameters())\n",
    "loss_func = nn.BCELoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the stopping criteria."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, Training Loss: 0.010312874983038222, Validation Loss: 0.009885733830928802\n",
      "Epoch: 2, Training Loss: 0.00819954081262861, Validation Loss: 0.006121420164903005\n",
      "Epoch: 3, Training Loss: 0.004075386614458902, Validation Loss: 0.0052307642281055455\n",
      "CPU times: user 3min 54s, sys: 55.8 s, total: 4min 50s\n",
      "Wall time: 5min 26s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "for epoch in range(1, epochs + 1):\n",
    "    running_loss = 0.0\n",
    "    running_corrects = 0\n",
    "    model.train() \n",
    "    for batch in train_iter: \n",
    "        \n",
    "        x = batch.text\n",
    "        y = batch.label\n",
    "        opt.zero_grad()\n",
    "        preds = model(x).squeeze()\n",
    "        loss = loss_func(preds, y.float())\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "        running_loss += loss.item()\n",
    "\n",
    "    epoch_loss = running_loss / len(trn)\n",
    "    \n",
    "    val_loss = 0.0\n",
    "    model.eval()\n",
    "    for batch in val_iter:\n",
    "        \n",
    "        x = batch.text\n",
    "        y = batch.label\n",
    "        \n",
    "        preds = model(x).squeeze() \n",
    "        \n",
    "        loss = loss_func(preds, y.float())\n",
    "        val_loss += loss.item()\n",
    "        \n",
    "    val_loss /= len(vld)\n",
    "    print('Epoch: {}, Training Loss: {}, Validation Loss: {}'.format(epoch, epoch_loss, val_loss))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate performance of the trained model (5 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "tp = 0\n",
    "fp = 0\n",
    "fn = 0\n",
    "tn = 0\n",
    "for batch in test_iter:\n",
    "    x = batch.text\n",
    "    y = batch.label\n",
    "    predictions = model(x).squeeze()\n",
    "    rounded_preds = torch.round(predictions)\n",
    "    confusion_vector = rounded_preds / y.float()\n",
    "    \n",
    "    \n",
    "    \n",
    "    tp += torch.sum(confusion_vector == 1).item()\n",
    "    fp += torch.sum(confusion_vector == float('inf')).item()\n",
    "    tn += torch.sum(torch.isnan(confusion_vector)).item()\n",
    "    fn += torch.sum(confusion_vector == 0).item()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy:  0.85732\n",
      "precision:  0.8618650247103622\n",
      "recall:  0.85104\n",
      "f1:  0.8564183069677576\n"
     ]
    }
   ],
   "source": [
    "accuracy = (tp+tn)/(tp+tn+fn+fp)\n",
    "print(\"accuracy: \", accuracy)\n",
    "precision = tp/(tp+fp)\n",
    "print(\"precision: \", precision)\n",
    "recall = tp/(tp+fn)\n",
    "print(\"recall: \", recall)\n",
    "f1 = 2*precision*recall/(precision+recall)\n",
    "print(\"f1: \", f1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write down the calculated performance\n",
    "\n",
    "### Accuracy:  0.8573\n",
    "### Precision: 0.8619\n",
    "### Recall: 0.8510\n",
    "### F1: 0.8564"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experiments (10 points)\n",
    "\n",
    "Experiment with the model and achieve better results. You can find advices [here](https://arxiv.org/abs/1801.06146). Implement and describe your experiments in details, mention what was helpful."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.\n",
    "### Use LSTM instead of GRU but use Adam optimizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMBaseline(nn.Module):\n",
    "    def __init__(self, vocab_size, hidden_dim, emb_dim, n_layers, device=device):\n",
    "        super().__init__()\n",
    "        self.vocab_size = vocab_size\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.emb_dim = emb_dim\n",
    "        self.n_layers = n_layers\n",
    "        self.embedding = nn.Embedding(num_embeddings=self.vocab_size, embedding_dim=self.emb_dim)\n",
    "        self.lstm = nn.LSTM(self.emb_dim, self.hidden_dim, num_layers=self.n_layers, dropout=0.5, bidirectional=True)\n",
    "        self.linear = nn.Linear(in_features=self.hidden_dim*2, out_features=1)\n",
    "        self.act = nn.Sigmoid()\n",
    "            \n",
    "    def forward(self, seq):\n",
    "        \n",
    "        emb = self.embedding(seq)\n",
    "        outputs, (hidden, cell) = self.lstm(emb)\n",
    "        hidden = torch.cat((hidden[-2,:,:], hidden[-1,:,:]), dim = 1)\n",
    "        outputs = self.linear(hidden)\n",
    "        preds = self.act(outputs)\n",
    "\n",
    "        return preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "em_sz = 200\n",
    "nh = 300\n",
    "vocab_size = len(TEXT.vocab)\n",
    "num_layers = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LSTMBaseline(\n",
       "  (embedding): Embedding(201435, 200)\n",
       "  (lstm): LSTM(200, 300, num_layers=2, dropout=0.5, bidirectional=True)\n",
       "  (linear): Linear(in_features=600, out_features=1, bias=True)\n",
       "  (act): Sigmoid()\n",
       ")"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = LSTMBaseline(vocab_size=vocab_size, hidden_dim=nh, emb_dim=em_sz, n_layers=num_layers, device=device); model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt = optim.Adam(model.parameters())\n",
    "loss_func = nn.BCELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LSTMBaseline(\n",
       "  (embedding): Embedding(201435, 200)\n",
       "  (lstm): LSTM(200, 300, num_layers=2, dropout=0.5, bidirectional=True)\n",
       "  (linear): Linear(in_features=600, out_features=1, bias=True)\n",
       "  (act): Sigmoid()\n",
       ")"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, Training Loss: 0.010667503012929644, Validation Loss: 0.010176380038261414\n",
      "Epoch: 2, Training Loss: 0.008569264927932195, Validation Loss: 0.008188266328970592\n",
      "Epoch: 3, Training Loss: 0.005305669948032924, Validation Loss: 0.0060926742573579155\n",
      "Epoch: 4, Training Loss: 0.00366311129118715, Validation Loss: 0.006672459904352824\n",
      "Epoch: 5, Training Loss: 0.0024167179827179227, Validation Loss: 0.005812775735060374\n",
      "CPU times: user 7min 2s, sys: 1min 44s, total: 8min 46s\n",
      "Wall time: 9min 46s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "for epoch in range(1, epochs + 1):\n",
    "    running_loss = 0.0\n",
    "    running_corrects = 0\n",
    "    model.train() \n",
    "    for batch in train_iter: \n",
    "        \n",
    "        x = batch.text\n",
    "        y = batch.label\n",
    "        opt.zero_grad()\n",
    "        preds = model(x).squeeze()\n",
    "        loss = loss_func(preds, y.float())\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "        running_loss += loss.item()\n",
    "\n",
    "    epoch_loss = running_loss / len(trn)\n",
    "    \n",
    "    val_loss = 0.0\n",
    "    model.eval()\n",
    "    for batch in val_iter:\n",
    "        \n",
    "        x = batch.text\n",
    "        y = batch.label\n",
    "        \n",
    "        preds = model(x).squeeze() \n",
    "        \n",
    "        loss = loss_func(preds, y.float())\n",
    "        val_loss += loss.item()\n",
    "        \n",
    "    val_loss /= len(vld)\n",
    "    print('Epoch: {}, Training Loss: {}, Validation Loss: {}'.format(epoch, epoch_loss, val_loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy:  0.8438\n",
      "precision:  0.8275783215184084\n",
      "recall:  0.86856\n",
      "f1:  0.8475740661227995\n"
     ]
    }
   ],
   "source": [
    "tp = 0\n",
    "fp = 0\n",
    "fn = 0\n",
    "tn = 0\n",
    "for batch in test_iter:\n",
    "    x = batch.text\n",
    "    y = batch.label\n",
    "    predictions = model(x).squeeze()\n",
    "    rounded_preds = torch.round(predictions)\n",
    "    confusion_vector = rounded_preds / y.float()\n",
    "    \n",
    "    \n",
    "    \n",
    "    tp += torch.sum(confusion_vector == 1).item()\n",
    "    fp += torch.sum(confusion_vector == float('inf')).item()\n",
    "    tn += torch.sum(torch.isnan(confusion_vector)).item()\n",
    "    fn += torch.sum(confusion_vector == 0).item()\n",
    "    \n",
    "accuracy = (tp+tn)/(tp+tn+fn+fp)\n",
    "print(\"accuracy: \", accuracy)\n",
    "precision = tp/(tp+fp)\n",
    "print(\"precision: \", precision)\n",
    "recall = tp/(tp+fn)\n",
    "print(\"recall: \", recall)\n",
    "f1 = 2*precision*recall/(precision+recall)\n",
    "print(\"f1: \", f1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.\n",
    "### Change number of layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LSTMBaseline(\n",
       "  (embedding): Embedding(202507, 200)\n",
       "  (lstm): LSTM(200, 300, num_layers=4, dropout=0.5, bidirectional=True)\n",
       "  (linear): Linear(in_features=600, out_features=1, bias=True)\n",
       "  (act): Sigmoid()\n",
       ")"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "em_sz = 200\n",
    "nh = 300\n",
    "vocab_size = len(TEXT.vocab)\n",
    "num_layers = 4\n",
    "model = LSTMBaseline(vocab_size=vocab_size, hidden_dim=nh, emb_dim=em_sz, n_layers=num_layers, device=device); model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LSTMBaseline(\n",
       "  (embedding): Embedding(202507, 200)\n",
       "  (lstm): LSTM(200, 300, num_layers=4, dropout=0.5, bidirectional=True)\n",
       "  (linear): Linear(in_features=600, out_features=1, bias=True)\n",
       "  (act): Sigmoid()\n",
       ")"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt = optim.Adam(model.parameters())\n",
    "loss_func = nn.BCELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, Training Loss: 0.010766579798289707, Validation Loss: 0.010535019485155742\n",
      "Epoch: 2, Training Loss: 0.010609476024763925, Validation Loss: 0.010886118125915528\n",
      "Epoch: 3, Training Loss: 0.01066178231239319, Validation Loss: 0.010738253450393677\n",
      "Epoch: 4, Training Loss: 0.010423775676318577, Validation Loss: 0.010440375026067098\n",
      "Epoch: 5, Training Loss: 0.008297995703560965, Validation Loss: 0.008229103203614552\n",
      "CPU times: user 15min 50s, sys: 5min 10s, total: 21min 1s\n",
      "Wall time: 22min 39s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "for epoch in range(1, epochs + 1):\n",
    "    running_loss = 0.0\n",
    "    running_corrects = 0\n",
    "    model.train() \n",
    "    for batch in train_iter: \n",
    "        \n",
    "        x = batch.text\n",
    "        y = batch.label\n",
    "        opt.zero_grad()\n",
    "        preds = model(x).squeeze()\n",
    "        loss = loss_func(preds, y.float())\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "        running_loss += loss.item()\n",
    "\n",
    "    epoch_loss = running_loss / len(trn)\n",
    "    \n",
    "    val_loss = 0.0\n",
    "    model.eval()\n",
    "    for batch in val_iter:\n",
    "        \n",
    "        x = batch.text\n",
    "        y = batch.label\n",
    "        \n",
    "        preds = model(x).squeeze() \n",
    "        \n",
    "        loss = loss_func(preds, y.float())\n",
    "        val_loss += loss.item()\n",
    "        \n",
    "    val_loss /= len(vld)\n",
    "    print('Epoch: {}, Training Loss: {}, Validation Loss: {}'.format(epoch, epoch_loss, val_loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy:  0.75804\n",
      "precision:  0.8401349783823685\n",
      "recall:  0.63736\n",
      "f1:  0.7248328253650549\n"
     ]
    }
   ],
   "source": [
    "tp = 0\n",
    "fp = 0\n",
    "fn = 0\n",
    "tn = 0\n",
    "for batch in test_iter:\n",
    "    x = batch.text\n",
    "    y = batch.label\n",
    "    predictions = model(x).squeeze()\n",
    "    rounded_preds = torch.round(predictions)\n",
    "    confusion_vector = rounded_preds / y.float()\n",
    "    \n",
    "    \n",
    "    \n",
    "    tp += torch.sum(confusion_vector == 1).item()\n",
    "    fp += torch.sum(confusion_vector == float('inf')).item()\n",
    "    tn += torch.sum(torch.isnan(confusion_vector)).item()\n",
    "    fn += torch.sum(confusion_vector == 0).item()\n",
    "    \n",
    "accuracy = (tp+tn)/(tp+tn+fn+fp)\n",
    "print(\"accuracy: \", accuracy)\n",
    "precision = tp/(tp+fp)\n",
    "print(\"precision: \", precision)\n",
    "recall = tp/(tp+fn)\n",
    "print(\"recall: \", recall)\n",
    "f1 = 2*precision*recall/(precision+recall)\n",
    "print(\"f1: \", f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
